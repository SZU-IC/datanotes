一、Spark
	①RDD
		（1）特性
	②DAG
		（1）什么是DAG？
		（2）如何划分DAG？
		（3）DAG如何生成？
	③调度系统
		（1）调度系统主要工作
		（2）调度系统核心组件
			ⅠDAGScheduler
			ⅡSchedulerBackend
			ⅢTaskScheduler
		（3）数据不动代码动
	④内存管理
	⑤通用性能调优
		（1）能省则省，能拖则拖
		（2）Tungsten优化
		（3）AQE优化
	⑥Shuffle性能杀手
		（1）Map和Reduce
		（2）reduceByKey和groupByKey
	⑦广播变量
		（1）普通变量
		（2）分布式数据集
		（3）问题，参数
	⑧CPU高效利用
		（1）并行度：spark.default.parallism、spark.sql.shuffle.partitions
			 并发度：spark.executor.cores决定
		（2）CPU低效的原因
			Ⅰ线程挂起：内存不够
			Ⅱ调度开销：分区数太多
	⑨内存视角
		（1）User Memory
		（2）Cache滥用
		（3）Storage Memory
	⑩磁盘视角和网络视角
		（1）作用：Shuffle阶段溢出临时文件、存储Shuffle阶段的中间文件、缓存分布式数据集
	
	SparkSQL性能调优
		1、RDD和DataFrame
			①RDD和DataFrame、DataSet的区别
		2、Catalyst逻辑计划优化
			①优化规则：谓词下推、列裁剪、常量替换
		3、Catalyst物理计划优化
			①优化Spark Plan：基于一些既定的规则将逻辑计划中的关系操作符一一映射成物理操作符。JoinSelection根据Join策略决定选择哪种Join
			②生成Physical Plan：Preparation Rules，如EnsureRequirements规则（添加Shuffle、Sort等）、ReuseExchange规则
		4、Tungsten钨丝计划
			①数据结构设计
			②基于内存页的内存管理
			③WSCG
		5、SparkSQL 3.0特性
			①AQE
				Ⅰ 优化触发的时机
				Ⅱ 特点：赖以优化的统计信息 优化决策
				Ⅲ Join策略调整 自动分区合并 自动数据倾斜处理
			②DPP
				Ⅰ条件
			③Join Hints
				Ⅰ单机模式下Join
				Ⅱ分布式模式下
				ⅢSpark如何选择Join策略

二、Hadoop
	1、HDFS
		①架构：NameNode、DataNode、SecondaryNameNode
		②特性
			主从架构、副本机制、分块存储、一次写入多次读取、分布式存储
		③读写流程
		④NameNode工作机制
		⑤DataNode工作机制
	2、MapReduce
		①架构：分而治之 Map、Reduce（setup、map/reduce、cleanup、run）
		②map数量 
			spiltSize = min(maxSize,max(minSize,blockSize))
		③reduce数量
			hive.exec.reducers.max = 1009
			hive.exec.reducers.bytes.per.reducer = 256MB
			set mapreduce.job.reduces = -1; 禁用
			设置参数无效场景：
				order by 、 笛卡尔积 、map端输出的数据量很小
		④MapReduce机制
			Ⅰ MapTask机制
				mapreduce.task.io.sort.mb = 100MB;
				mapreduce.map.sort.spill.percent = 0.8;	溢写的内存占比
				mapreduce.task.io.sort.factor = 10; 一次合并溢写文件数
			
			Ⅱ ReduceTask机制
				Copy阶段
				Merge阶段
				Reduce阶段
			Ⅲ Shuffle机制
				Collect阶段
				Spill阶段
				Merge阶段
				Copy阶段
				Sort阶段
	3、YARN
		①组件：RM(Scheduler、ApplicationManager)、NodeManager、ApplicationMaster、Container
		②调度策略

三、Hive
	1、架构和工作流程
		架构：客户端、Driver端、解析器、编译器、优化器、执行器
		执行引擎：MR、Spark、Tez
			Spark：Spark on Hive（操作Hive底层源数据）、Hive on Spark
	2、分区表
		①参数
			hive.exec.dynamic.partition=true;
			hive.exec.dynamic.partition.mode=nonstrict;
			hive.exec.dynamic.partition.node=1000;	//最多可以创建的动态分区
			hive.exec.dynamic.partition.pernode=100; 每个执行MR节点最多可以创建的动态分区
			hive.exec.max.created.files=100000;	整个MR JOB中可以创建的文件数
		②strict模式限制
			对分区表查询，where中过滤字段不是分区字段
			笛卡尔积join查询，没有on条件
			order by查询，不带limit
	3、分桶表
		①参数：cluster by(id) into 503 buckets
		②分区是分目录，分桶是份文件。
		③注意：
			reduce的个数设置大于等于分桶数。
			分桶表只能通过insert overwrite的方式将普通表的数据加载到分桶表中。
			招证中的inceptor支持事务，因此表存储格式需要为orc。
	4、自定义UDF函数
		①UDF函数：继承UDF（Java）、GenericUDF（scala）  --UDAF、UDTF类似
		②高阶函数
			窗口函数：row_number(不同、不空)、rank(同、会有空位)、dense_rank(同、不会有空位)、first_value()、last_value()
					  lead(col,n,default)取窗口内往下第n行值、lag(col,n,default)、collect_set()、collect_list()
					  sort_array(collect_list(field))	对数组进行排序，通常结合collect_set()和collect_list()使用
					  over(partition by field1 order by field2)
					  CURRENT_ROW(),n PRECEDING,UNBOUNDED PRECEDING,N FOLLOWING,UNBOUNDED FOLLOWING
					  OVER(ORDER BY score desc rows 2 PRECEDING) AS avg_score
			JSON相关：get_json_object(field,return key)
			列转行：explode ，lateral view explode(expression) tableAlias as columnAlias
			行转列：concat_ws(),concat()
			判断函数：case when then else end
	5、表的优化
		①小表Join大表（MapJoin）
			设置参数自动选择MapJoin：set hive.auto.convert.join=true;
			大小表阈值设置：set hive.mapjoin.smalltable.filesize=25000000;
		②大表Join大表
		   Ⅰ 过滤无效数据、对业务没有影响数据（空key数据）
		   Ⅱ 分而治之：是否是某些key的数据太多，可以单独拿出来进行打散
	6、优化 
		①表层面优化
			分区表
			分桶表（主要用于join、采样）
				CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
				两个表以相同的方式分桶（默认是hash），会提高Join速度
			②选择合适的文件存储格式
				TextFile：行存储，默认格式；可结合Gzip、Bzip2等压缩方式使用（推荐可切分压缩方式）
				SequenceFile：二进制，可分割可压缩（RECORD、BLOCK）
				RCFile：数据块按行分块，每块按列存储
				ORCFile：数据块按行分块，每块按列存储；会基于列创建索引，查询速度比较快；
				ParquetFile：列式存储；对于大型查询类型是高效的，特别是扫描特定列；一般使用Snappy（默认）、Gzip压缩；支持Impala查询引擎
			③选择合适的压缩格式
				压缩格式	是否可拆分	是否可自带	压缩率	速度	是否hadoop自带
				gzip		否			是			很高	比较快	是 
				lzo			是			是 			比较高	很快	否
				snappy		否			是			比较高  很快    否
				bzip2		是 			否			最高	很慢	是 
		②HQL层面优化
			Ⅰ 执行计划：explain select * from tableName;
		    Ⅱ 列、行、分区裁剪
				参数：set hive.optimize.cp = true; 列剪裁，只取查询中需要的列；
					  set hive.optimize.pruner = true;分区裁剪
			Ⅲ 谓词下推 ：使SQL语句中where逻辑尽可能提前执行，减少下游处理的数据量。
				参数：set hive.optimize.ppd=true;
			Ⅳ 合并小文件
				参数：
					Map端输入合并
						set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
						set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;		#不合并
				    Map/Reduce端输出合并
						set hive.merge.mapfiles=true;
						set hive.merge.mapredfiles=true;
						set hive.merge.size.per.task=256000000;	#合并文件大小
						set mapred.max.split.size=256000000;
						set mapred.min.split.size.per.node=1; 	//一个节点上split的最少值
						set mapred.min.split.size.per.rack=1;	//一个机架上split的最少值
			Ⅴ 合理设置MapTask并行度
				splitSize = max(minSize,min(maxSize,blockSize))
				mapreduce.input.fileinputformat.split.minSize=1;
				mapreduce.input.fileinputformat.split.maxsize=256;
			Ⅵ 合理设置ReduceTask并行度
				ReduceTask过多，产生大量的小文件，对HDFS造成压力。
				ReduceTask过小，单个ReduceTask可能处理的数据量过多，易产生数据倾斜，或查询较慢。
				参数1：hive.exec.reducers.bytes.per.reducer=256M;
				参数2：hive.exec.reducers.max=1009;
				参数3：mapreduce.job.reducers=-1;(默认-1，由上面两个参数进行设置)
				N = Math.min(参数2，总输入数据大小 / 参数1);
			Ⅶ Join优化
				Join的整体优化原则：
					优先过滤后再进行Join操作，最大限度的减少参与Join的数据量。
					小表Join大表，最好启动MapJoin，hive自动启用MapJoin，小表不能超过25M，可以更改。
					Join On的条件相同的话，最好放入同一个Job，并且Join表的排列顺序从小到大。
					如果多张表做Join，如果多个链接条件都相同，会转换成一个Job。
				优先过滤数据
					尽量减少每个阶段的数据量。
				小表Join大表 
					将小表放在左边，hive.auto.convert.join=true;
				大表Join大表
			八 SMBJ（Sort-Merge-Bucket Join）
				要求：
					针对参与Join的两张表要做相同的hash散列，每个桶里面的数据要排序。
					两张表的分桶个数要成倍数。
					开启SMB Join的开关。
				参数：
					set hive.enforce.sortmergebucketmapjoin=false;	#当用户执行bucket map join时，发现不能执行时，禁止查询。
					set hive.auto.convert.sortmerge.join=true;	    #如果join的表通过sort merge join条件，join是否会自动转换为sort merge join；
					## 当两个分桶表 join 时，如果 join on的是分桶字段，小表的分桶数是大表的倍数时，可以启用 mapjoin 来提高效率。 
					# bucket map join优化，默认值是 false set hive.optimize.bucketmapjoin=false; 
					## bucket map join 优化，默认值是 false set hive.optimize.bucketmapjoin.sortedmerge=false;
			Ⅸ Join数据倾斜处理
				参数设置：
					# join的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置 
						set hive.skewjoin.key=100000; 
					# 如果是join过程出现倾斜应该设置为true 
						set hive.optimize.skewjoin=false;
					#如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认100000）的倾斜 key 对应的行临时写进文件中，
					然后再启动另一个 job 做 map join 生成结果。
						set hive.skewjoin.mapjoin.map.tasks=10000;
			Ⅹ Group By优化
				1、Map端部分预聚合
					set hive.map.aggr=true;
					#设置map端预聚合的行数阈值，超过该值就会拆分Job，默认值为100000；
					set hive.groupby.mapaggr.checkinterval=100000;
				2、有数据倾斜实现负载均衡（在第一个 MapReduce 任务中，map 的输出结果会随机分布到 reduce 中，每个 reduce 做部分聚合操作，
				    并输出结果，这样处理的结果是相同的`group by key`有可能分发到不同的 reduce 中，从而达到负载均衡的目的； 
					在第二个 MapReduce 任务再根据预处理的数据结果按照 group by key 分布到各个 reduce 中，最后完成最终的聚合操作。）
					set hive.groupby.skewindata=false;	//先预聚合后汇总
			ⅩⅠ Order By 
				1、区别
					order by只启动一个reduce
					sort by单个reduce结果有序
					cluster by：对于同一字段分桶并排序，不能和sort by连用
					distribute by + sort by
					
Parquet和ORC的存储区别：
	Parquet支持嵌套格式，ORC不是很支持。
	ORC的读取速度和压缩比率都比Parquet好，都是二进制存储
	Parquet：Row Group、Column Chunk、Data Page  Parquet是为了使Hadoop生态系统中的任何项目都可以使用压缩的，高效的列式数据表示形式
		parquet.block.size=128M  Row Group在内存中的块大小
		parquet.page.size=1MB,标识每个页的大小（压缩完后）
		parquet.dictionary.page.size 字典页大小 1MB，启用后有重复数据可以进行压缩。
	ORC：    Row Data（10000行）、 Column  ---》 ORC提供了三级索引，文件级（stripe footer数据所在的文件目录）、条带级（index Data）、行组级（row Data）

ORC存储支持Snappy压缩，但是Snappy不支持切分，底层跑MR任务时不能切分数据块，容易导致数据倾斜
create table stu_orc(id int,name string)
	stored as orc 
	tblproperties ('orc.compress'='snappy');		-- orc默认是zlib

create table stu_par(id int,name string)
	stored as parquet 
	tblproperties ('parquet.compression'='lzo');			#可以使用parquet+lzo进行存储和压缩  （lzo和bzip2都可以进行分割）

create table stu_par(id int,name string)
	stored as parquet 
	tblproperties ('parquet.compression'='snappy');

SQL优化案例：
	上游有一张增量表，每天一个增量分区，使用的时候有很多task，后来添加了combineHiveInputFormat参数，效率从30min提升到20min，同时向上游反馈。
	还有一些是某些task执行特别慢，查看了task的input数据量，发生了数据倾斜。
	小表关联大表使用了mapjoin。

数仓中，除了接口表（从其他数据库导入或者最后要导出到其他数据库的表），其余表的存储格式与压缩格式保持一致。



四、HBase
	1、基础
		①数据模型：稀疏、分布式、多维、排序的映射
			逻辑结构：存储稀疏、数据存储多为，不同的行具有不同的列。数据存储整体有序，按照RowKey的字典序排列，RowKey为Byte数组。
				Row Key：行号（排序） 
				Column Family：列族,切分store
				Region：按照RowKey拆分
				Store：竖向切分划分（列划分）
			物理存储：(StoreFile)
				Key：RowKey + ColumnFamily + Column Qualifier + TimeStamp + Type（Put/Delete）
				Value
				HDFS不能修改数据，根据时间戳版本标记，读取最新的版本
			
			NameSpace（Database）：default、hbase（内置表）
			Table：定义表时只需要声明列族即可，不需要声明具体的列，可以动态、按需指定。
			Row：RowKey+多个Column，只能根据RowKey进行检索
			Column：Column Family + Column Qualifier（列限定符）
			TimeStamp：标识数据版本，每条数据写入时，系统会自动为其加上该字段。
			Cell：{rowkey+column Family：column Qualifier，timestamp}，唯一确定单元
		②基础架构：主从架构 
			Master
				hbase:meta存储在hdfs，有master管理。
				主要进程，通过ZK监控RS进程状态，启动监控region是否需要负载均衡，故障转移和region拆分。
			BackUp-Master
			Zookeeper
				master高可用，记录RegionServer的部署信息，并且存储meta表的位置。
			RegionServer
		③基本操作
			Shell操作
			Java客户端连接操作
			Spark操作
	2、进阶
		①Master架构
			Ⅰ Master实现类HMaster，通常部署在NameNode上。
				RegionServer将自己的信息注册到Zookeeper上，Master直接找Zookeeper去管理RegionServer
				元数据表meta
				负载均衡器调节	-- 	Region负载均衡，通过Zk了解RS的启动情况，5分钟调控看过一次。
				元数据表管理器  --  管理meta表
				MasterProcVAL预写日志 -- 防止Master挂掉，先写到预写日志，挂掉后可以通过Backup Master来完成	/hbase/MasterData/WALs
									  -- 避免小文件，32M文件或者1h滚动，当操作执行到meta表之后删除WAL。
				Meta表介绍：
					RowKey: {[table],[region start key],[region id]}
					列：	{info:regioninfo,info:server,info:serverstartcode}
				Admin：连接Master	Table：连接Zookeeper
		②RegionServer：实现类 HRegionServer，通常部署在DataNode
			Ⅰ WAL预写日志
				保证有序
			Ⅱ BlockCache 读缓存，只有一个
				
			Ⅲ MemStore 写缓存，需要排序（每个Store有单独一个）
		③写流程 
			Client --> Zookeeper 发送连接请求，读取meta表的地址。
			读取meta表的数据缓存到 MetaCache（几MB） --> Meta表发生变化需要重新读取
			发送put请求，将请求写到WAL并落盘
			操作put请求写入到对应的MemStore并排序
			等待触发刷写条件写入到对应的store（只能保证单文件有序）

			Ⅰ、先写到WAL原因：数据在MemStore会排序并保留一段时间，存在内存中不安全，数据写入MemStore之后就会返回成功的ACK，此时宕机可以通过WAL找回。
			Ⅱ、MemStore Flush(资源充足，效率最高)
				某个memstore大小：hbase.hregion.memstore.flush.size=128M，其所在region的所有memstore都会刷写（为了保证不同列族可以一起刷写）
				memstore的大小达到 
					hbase.hregion.memstore.flush.size(128M) * hbase.hregion.memstore.block.multiplier(默认为4)
					刷写同时阻止继续往该memstore写数据（由于线程监控是周期性的，所以有可能面对数据洪峰，尽管可能性比较小）
			Ⅲ、LOWER_MARK(低水位) 和 HIGH_MARK(高水位)	（资源可能不那么充足）	-->  由HRegionServer中的属性MemStoreFlusher内部线程FlushHandler控制，避免写太多造成OOM
				LOWER_MARK:
					JAVA_HEAPSIZE * hbase.regionserver.global.memstore.size（默认值0.4）* 
					hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）
				HIGH_MARK
					JAVA_HEAPSIZE * hbase.regionserver.global.memstore.size（默认值0.4）	--   阻止往里面写数据
			Ⅳ、刷写时间（1h）（写入数据量太少）	-->  HRegionServer的属性PeriodicMemStoreFlusher控制
					hbase.regionserver.optionalcacheflushinterval（默认1小时）
		
		④ HFile结构 -- 每一个store文件夹下实际存储数据的文件
			数据本身Key 、Value 
				rowlength、row、columnfamilylength、columnfamily、columnqualifier、timestamp、keytype（put）
			元数据记录
			文件信息
			数据索引
			元数据索引
			一个固定长度的尾部信息（版本信息）
			通过命令查看：bin/hbase hfile -m  -f /hbase/data/命名空间/表名/regionID/列族/HFile名
		⑤读流程
			客户端创建连接，向ZK发送请求，请求meta表地址，向RegionServer读取meta表数据到MetaCache
			发送get请求，将请求写入WAL并落盘
			读取BlockCache（尾部信息--对比信息是否一致）
			读取对应写缓存和store文件，同时缓存到BlockCache
			合并所有读取数据返回（只取最高版本）

			合并读取数据优化：
				HFile带有对应的索引文件，读取对应Rowkey数据会比较快。
				BlockCache会缓存之前读取的内容和元数据信息，如果HFile没有发生变化（记录在HFile为信息中），则不需要再次读取
				使用布隆过滤器过滤当前HFile中不需要读取的RowKey。
		⑥ StoreFile Compaction
			Minor Compaction（从旧到新，一般小合并和大合并一起工作，默认3个HFile） -- 在Minor Compaction会保留deleteAll
				hbase.hstore.compaction.ratio（默认1.2F）合并文件选择算法中使用的比率。
				hbase.hstore.compaction.min（默认3）  为Minor Compaction的最少文件个数。
				hbase.hstore.compaction.max（默认10） 为Minor Compaction最大文件个数。
				hbase.hstore.compaction.min.size（默认128M）为单个Hfile文件大小最小值，小于这个数会被合并。
				触发条件：
					过小合并，过大不合并
					文件大小/ hbase.hstore.compaction.ratio<剩余文件大小和 则参与压缩 。不建议修改ratio
					满足压缩条件的文件个数达不到个数要求（3<=count<=10）则不压缩
			Major Compaction（将一个Store下所有HFlie合并成一个大文件）
				默认7天
		⑦ Region Split
			Ⅰ、预分区（自定义分区）
				create 'staff1','info', SPLITS => ['1000','2000','3000','4000']  --> 5个Region
				create 'staff2','info',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
				create 'staff3', 'info',SPLITS_FILE => 'splits.txt'	 ---> 按照文件设置的规则预分区
			Ⅱ、系统分区
				当前的RegionServer上该表只有一个Region，按照 2 * hbase.hregion.memstore.flush.size(128M)切分
				否则按照hbase.hregion.max.filesize(10G)分裂
	3、优化
		① RowKey设计
			生成随机数、hash、散列值
			时间戳反转（旧的会写在前面，新的在后面 99999999-时间戳）
			字符串拼接

			Ⅰ 组合实现（要根据需求设置）
			可以穷举的写在前面
			rowkey设计格式 => date(yyyy-MM)^A^Auserdate(-dd hh:mm:ss)
				scan: startRow
					  stopRow
			Ⅱ 添加预分区
				rowkey设计格式 => 分区号date(yyyy-MM)^A^Auserdate(-dd hh:mm:ss)
		②参数优化
			Ⅰ Zookeeper会话超时时间：zookeeper.session.timeout=90000ms
				当一个RegionServer挂掉，90s后Master才能发现，可以调到20~30s
			Ⅱ Major Compaction大合并 （默认7天）
			Ⅲ HStore文件大小
				hbase.hregion.max.filesize=10GB(可以调大，现在一般不跑mr)
			Ⅳ 优化HBase客户端缓存
				hbase.client.write.buffer=2M 
			Ⅴ Scan扫描HBase所获取的行数
			Ⅵ 读缓存和写缓存
				hfile.block.cache.size=0.4
				hbase.regionserver.global.memstore.size=0.4
				读请求比较多提高读缓存比例，写缓存比较多提高写缓存比例。
		③JVM调优
			Ⅰ 内存设置
			Ⅱ 垃圾回收器设置
		④HBase经验法则
			Ⅰ Region大小控制在10-50G
			Ⅱ cell大小不超过10M
			Ⅲ 1张彪有1到3个列族，不要设计太多。最好1个，如果使用多个尽量保证不会同时读取多个列族。
			Ⅳ 1到2个列族的表格，设计50-100个Region。
			Ⅴ 列族名称要尽量短。
			Ⅵ 如果RowKey设计时间在最前面，会导致有大量的旧数据存储在不活跃的Region中，使用的时候，仅仅会操作少数的活动Region，此时建议增加更多的Region个数。
			Ⅶ 如果只有一个列族用于写入数据，分配内存资源的时候可以做出调整，即写缓存不会占用太多的内存。

	5、Phoenix二级索引
		① 全局索引
			创建全局索引时，会在HBase中建立一张新表，索引和数据表存放在不同的表中，适合多读血少的业务场景。
			如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。
		② 解决全局索引的问题
			包含索引（covered index）
			create index my_index on my_table(v1) include(v2)

			本地索引 （适合频繁写）
				索引数据和数据表的数据是存放在同一张表中（且是同一个Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。
				CREATE LOCAL INDEX my_index ON my_table (my_column);	-- my_column可以写多个
	
	6、Hive集成HBase
		CREATE TABLE hive_hbase_emp_table(
    		empno int,
    		ename string,
    		job string,
    		mgr int,
    		hiredate string,
    		sal double,
    		comm double,
    		deptno int
		)
		STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
		WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")
		TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");

		需要使用insert into 插入不能使用load data into 

		在Hive中创建外部表映射到HBase的表中
		CREATE EXTERNAL TABLE relevance_hbase_emp(
    		empno int,
    		ename string,
    		job string,
    		mgr int,
    		hiredate string,
    		sal double,
    		comm double,
    		deptno int
		)
		STORED BY 
		'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
		WITH SERDEPROPERTIES ("hbase.columns.mapping" = 
		":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") 
		TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");


五、Kafka
	1、基础
		①概述
			Ⅰ 特点：高吞吐、低延迟、分布式存储、容错性、可拓展性、高并发、发布订阅模式
			Ⅱ 使用场景
				消息推送、监控、告警、流式处理、缓存、缓冲/解耦、削峰
			Ⅲ 架构 
				Producer ：消息不丢失，至少一次
				Consumer：断点续传，记录消费的offset。拉取模式
				Broker
				Topic
				Partition：单个partition内有序
				Leader
				Follower
				Controller
			Ⅳ 使用命令
				topic：bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first
				producer：bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first
				consumer：bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first		--从头开始读
			Ⅴ 生产者（发送消息）
				Producer 
					main线程：将消息发送给双端队列：RecordAccmulator（32M）
						Interceptor
						Serializer
						Partitioner
					Sender线程：不断从RecordAccmulator队列拉取消息发送到Kafka Broker
						send 
					参数：
						batch.size: 默认是16K，适当增加该值提高吞吐量。但是设置太大会导致数据延迟。
						linger.ms： 默认为0ms，建议设置为5-100ms。
						buffer.Memory:RecordAccmulator的大小，默认32M
						acks：-1，0，1
						retries：重试次数
						retry.backoff.ms:两次重试的时间间隔，默认是100ms
						enable.idempotence:默认为true，开启幂等性
					生产者分区partitioner：
						提高并行度：生产者以分区为单位发送数据，消费者以分区为单位消费数据。
						便于合理使用存储资源：实现负载均衡。
						
						可以自定义Partitioner
					如何提高吞吐量：
						bacth.size = 16K
						linger.ms = 默认为0，修改为5-100ms。
						buffer.Memory: RecordAccumulator，默认32M。
						compression.type: 默认为none，可配置为gzip，snappy，lz4。
					数据可靠性：
						acks = -1：需要等到leader和ISR中副本同步完全。
						分区副本大于等于2
						ISR里应答的最少副本数量大于等于2， 如果分区副本为1或min.insync.replicas默认为1，仍然有丢数据的风险。
					数据不重复（因为重试机制）：
						Kafka中的数据语义：
							至少一次：ack=-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2
							最多一次：ack = 0
							精确一次：至少一次 + 幂等性 <PID,Partition,SeqNumber>:PID为机器重启都会生成一个。Partition：分区号；SeqNumber：单调递增
								幂等性只能保证单分区单会话内不重复。
						Kafka事务：开启事务必须开启幂等性 (保证多分区会话的不重复)
							initTransactions()
							beginTransaction()
							sendOffsetsToTransaction()
							commitTransaction()
							abortTransaction()

					数据乱序：
						1.x版本之后保证单分区有序
							未开启幂等性：max.in.flight.requests.per.connection=1
							开启幂等性： max.in.flight.requests.per.connection设置小于等于5（会缓存最近5个requests的元数据）
			
			Ⅵ Broker
				① 参数
					log.segment.bytes = 1G 
					log.index.interval.bytes = 4kb，kafka里面每当写入了4kb大小的日志（.log）,就会往index文件里面记录索引。
					log.retention.hours=168,默认7天 
				②节点服役和退役
			Ⅶ Kafka副本（一般2个）太多占用太多磁盘，网络传输
				基本信息：
					AR = ISR + OSR 
					读写数据只会往leader上读写，Follower长时间没有向Leader发送同步数据命令就会被踢出ISR replica.lag.time.max.ms=30是
				Leader选举
					broker启动后向zk注册，每隔broker都有一个controller，谁先注册，谁说了算。
					controller决定leader选举，根据AR的顺序进行选举（在ISR中存活）
				Follower故障恢复
					LEO：最新的offset+1，下一个写的位置
					HW：所有副本中最小的LEO+1

					Follower将自己高于本地记录的HW的记录截取掉，，从HW开始向Leader同步
				Leader故障
					从ISR选举出一个新的Leader
					为了保证数据一致性，其余的Follower会先将各自的log高于HW的截取掉，然后再和新Leader同步。（不能保证不丢失）

				分区副本分配：负载均衡
					手动调整分区副本
					Leader Partition负载均衡：如果某些机器宕机，重启后可能导致负载不均衡。
						auto.leader.rebalance.enable=true;
						leader.imbalance.per.broker.percentage=10%
						leader.imbalance.check.interval.seconds=300s
					增加副本因子
						命令行：bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute
			Ⅷ Kafka文件存储机制
				Topic -> Partition -> log -> Segment(1G) {.log,.index,.timeindex（用来删除日志，默认7天）}
				index和log文件是以当前segment的第一条消息的offset命名			
				index为稀疏索引，每往log文件存储4k数据，会往index文件写入一条索引。log.index.interval.bytes=4kb 	log.segment.bytes=1G 
				index中保存的索引是相对offset
			
			Ⅸ 文件清理策略
				kafka默认的日志保存时间为7天
					log.retention.hours=168
					log.retention.minutes
					log.retention.ms 
					log.retention.check.interval.ms 负责设置检查周期，默认5min
				delete策略
					log.clean.policy = delete 
					基于时间：默认打开。以segment中所有记录中的最大时间戳作为该文件时间戳。
					基于大小：默认关闭。超过设置的所有日志总大小，删除最早的segment。	-- log.retention.bytes=-1,表示无穷大
				compact策略
					log.clean.policy = compact ，所有数据启用压缩策略，对于key 不同的value值，只保留一个版本
					压缩后的offset可能不连续
			Ⅹ 高效读写数据
				Kafka本身是分布式集群，可以采用分区技术，并行度高
				读数据采用稀疏索引，可以快速定位要消费的数据
				顺序写磁盘
				页缓存 + 零拷贝
					页缓存将操作系统多余的内存当作磁盘的缓存来使用
					零拷贝不经过socket Cache，直接到网卡
		
		② Kafka消费者
			1、消费方式
				pull ：可以按需拉取，但Kafka没有数据可能陷入循环，一直返回空数据。
			2、消费者工作流程
				Ⅰ 消费者组
					Consumer Group
						所有消费者的 groupid 相同
						消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者组内的一个消费者消费。
						消费者组之间不相互影响。
				Ⅱ Coordinator:辅助实现消费者组的初始化和分区的分配（每个broker都有）
					coordinator节点选择 = groupid 的hashcode值 % 50 （__consumer_offset的分区数量），对应的分区所在的节点的coordinator作为消费者组的老大
				Ⅲ 工作流程
					找到coordinator
					每隔consumer发送JoinGroup请求
					选出一个consumer作为leader
					把要消费的topic情况发送给leader消费者
					把消费方案发给coordinator
					Coordinator把消费方案发给各个consumer

						每个消费者都会和coordinator保持心跳（默认3s），一旦超时 session.timeout.ms=45s,该消费者会被移除，并触发再平衡
						或者消费者处理消息的时间过长（max.poll.interval.ms=5min），也触发再平衡。
				Ⅳ 消费流程
						ConsumerNetWorkClient
						参数：
							fetch.min.bytes = 1byte
							fetch.max.bytes = 50M
							max.poll.records = 500条	-- 一次拉取返回消息最大条数
							auto.offset.reset (Kafka中没有初始偏移量或当前偏移量在服务器中不存在（如数据被删除）)
								eraliest:自动重置偏移量到最早的偏移量
								latest：默认，自动重置偏移量为最新的偏移量。
								none：如消费者组原来的偏移量不存在，则向消费者抛异常。
							offset.topic.num.partitions：__consumer_offsetts的分区数，默认为50个
							heartbeat.interval.ms：Kafka消费者和coordinator之间的心跳时间，默认是3s
							session.timeout.ms = 45s 消费者和coordinator超时的时间
				Ⅴ 分区分配以及再平衡
					分区分配策略： partition.assignment.strategy,3.0默认是Range + CooperativeSticky
						Range ：partition数 / consumer数（针对单个topic来处理）		-- 多个topic容易产生数据倾斜，分区数只能增加，不能减少
						RoundRobin：轮询分配，针对集群中所有topic而言，把所有的partition和所有的consumer都列出来，按照hashcode进行排序，通过轮询算法来分配partition给消费者。
						Sticky ：partition数 / consumer数（随机分配partition，区别于range），重分配时将挂掉的消费者消费分区分配给剩下消费者
						CooperativeSticky
				Ⅵ offset维护
					offset维护的位置：__consumer_offsets
						__consumer_offsets主题采用key和value的方式存储数据。key：groupid + topic + 分区号，value：当前的offset值。
						自动提交offset：
						enable.auto.commit :是否自动提交offset功能，默认true。
						auto.commit.interval.ms:自动提交offset得时间间隔，默认5s。
						手动提交offset：
							commitSync：同步提交，必须等到offset提交完成只会，才能进行下一次提交。
							commitAsync：异步提交
						指定offset进行消费：
							auto.offset.reset = eariest(--from-beginning) | latest | none 
							seek(topicpartition,offset);
								消费者分配分区策略需要较长时间，因此需要等到分区分配完成才能指定位置消费
						指定时间消费
				Ⅶ 重复消费和漏消费
					重复消费：自动提交offset引起，consumer没5s提交offset，提交offset2s后，consumer挂掉，再次重启consumer就会从上一次提交offset处继续消费，导致重复消费。
					漏消费：手动提交，offset被提交时，数据还在内存中还没落盘，此时刚好消费者线程被kill掉，那么offset已经提交，但数据未处理，导致这部分内存中得数据丢失。

					解决：消费者事务
				Ⅷ 数据积压
					Kafka消费者消费能力不足：
						增加Toipc分区数，增加消费者数。
					下游得数据处理不及时：
						提高每批次拉取得数量。批次拉取得数据过少，使得处理的数据小于生产的数据。

六、ElasticSearch
	1、概述
		ELK：ES + Logstash + Kibana
		全文检索搜索引擎
			搜索并查询分析
			分布式，可扩展
			
	2、进阶
		① 核心概念
			Ⅰ Index 索引 
				能搜索的数据必须索引，这样的好处是可以提高查询速度。
				ElasticSearch索引的精髓：一切设计都是为了提高搜索的性能。
			Ⅱ Type 类型
				7.x之后版本不支持自定义类型，默认类型为_doc
			Ⅲ Document 文档
				一个文档是一个可被索引的基础信息单元，也就是一条数据。
				一个index/type里面可以存储任意多个文档。
			Ⅳ Field 字段
				相当于是数据表的字段，对文档数据根据不同属性进行的分类标识。
			Ⅴ Mapping 映射
				ES里面数据的一些使用规则设置也叫做映射，按着最优规则处理数据对性能提高很大。
				类似表结构
			Ⅵ Shards 分片
				分片原因：
					允许水平分割 / 扩展内容容量。
					提高分布式、并行操作，进而提高性能/吞吐量。
				一个分片就是一个Lucene索引，一个ElasticSearch索引就是分片的集合。
				当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后合并每个分片的结果到一个全局的结果集。
			Ⅶ Replicas 副本
				副本原因：
					在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。
					扩展搜索量/吞吐量，搜索可以在任何副本上并行运行。
				默认：1个分片 1个副本
			Ⅷ Allocation 分区分配
				将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。
				这个过程是由master节点完成的。
		② 系统架构
			ElasticSearch集群：由一个或多个拥有相同cluster.name配置的节点组成，共同承担数据和负载的压力。
			Master节点：
				负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点。
				主节点不需要设计到文档级别的变更和搜索等操作，因此流量的增加Master也不会称为瓶颈。
			
			普通节点:
				每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。

			故障转移：
				只要有相同的cluster.name，他就会发现集群并自动加入其中。
				新的节点加入，会将分配重新分配到节点上。
			
			水平扩容：
				可以动态调整副本分片数目，按需伸缩集群。
			
			应对故障
				主节点故障后，需要选举新的节点。
				将故障主节点上的主分片对应的副本分片提升为主分片。
			
			路由计算：
				公式：shard = hash(rounting) % number_of_primary_shards
				rounting：默认是文档的 _id ,也可以设置为自定义的值。
				number_of_primary_shards：分片数量。 （主分片数量）
				所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，
				通过这个参数我们可以自定义文档到分片的映射。
			
			分片控制：
				协调节点：接收请求的节点，为了实现负载均衡，可以进行轮询集群中的所有节点。
				写流程：
					1.客户端向其中一个节点发送新建、索引或者删除请求。
					2.节点使用文档的 _id 确定文档属于分片S。请求会被转发到拥有S主分片的节点N。
					3.N节点在主分片上执行请求。如果执行成功，将请求并行转发到其他节点的副本分片上。一旦所有的副本分片都报告成功，
					N节点将向协调节点报告成功，协调节点向客户端报告成功。
					参数：
						consistency:即一致性。 必须要有大多数）的分片副本处于活跃可用状态，才会去执行_写_操作(其中分片副本可以是主分片或者副本分片)
						one (只要副本分片状态ok就可以执行写操作) all(主分片和副本分片状态都ok) quorum(默认设定值)
						quorum = int( (primary + number_of_replicas) / 2 ) + 1

						timeout: 默认等待1min

				读流程：
					客户端向N节点发送获取请求。
					节点使用文档的id来确定文档属于的分片，并确认副本分片所在的节点。将请求转发到对应的节点N2。
					N2节点集合将文档返回给N，N进行整理将文档返回给客户端。

					注意：
						文档可能在主分片已经写入了，但是副本分片还没有写入。副本分片就会报文档不存在，但是主分片成功返回了。
						一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。
				
				更新流程：
					客户端向N节点发送更新请求。
					将请求转发到文档所在的主分片节点N2。
					N2节点检索文档，修改_source字段中的JSON，并城市重新索引主分片的文档，如果文档已经被另一个进程修改，会进行重试，超过retry_on_conflict次数后放弃。
					N2节点成功更新文档，将新版本的文档转发到其他节点的副本分片上，重新建立索引。一旦所有副本分片都返回成功了，N2节点向协调节点返回成功，协调节点返回给客户端。

					注意：当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，
					并且不能保证它们以发送它们相同的顺序到达。如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。

				多文档操作：
					mget 和 bulk API的模式类似于单文档模式。
					区别在于协调节点知道每个文档存在于哪个分片中。它将整个多文档请求分解成 每个分片 的多文档请求，并且将这些请求并行转发到每个参与节点。
					协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。

					步骤：
						客户端向节点Node1发送mget请求
						Node1为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或副本分片的节点上，一旦收到所有的答复，Node1构建响应并将其返回给客户端。

			分片原理： 一个分片就是一个Lucene执行引擎
				倒排索引
					倒排索引： 关键词 ---> 文档id
					正向索引： 文档id ---> 关键词
					倒排索引被写入磁盘后是不可改变的，有个版本的字段，每次取最新的。

				动态更新索引（如何保留不变性的前提下实现倒排索引的更新？）
					用更多的索引，通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。
					每一个倒排索引都会被轮流查询到，从最早的开始查询完后再对结果进行合并。

					分片文件存储： 所有段集合 + 提交点（列出所有已知段文件）
					文件更新时，会将旧段中的文档删除，并在提交点包含一个.del文件，文件会累出这些被删除文档的段信息。（删除同理）

			近实时搜索
				refresh_interval ： 1s
				在创建一个大的索引时，而不是为了近实时搜索时，可以将refresh_interval=-1 关闭
				项目中用了 refresh_interval ： 600s ，为了提高写速度

			持久化变更：
				translog:事务日志，在每一次对ElasticSearch进行操作时均进行日志记录。
				大小为512M，写满会触发Flush。
			
			segment合并
				POST your_index/_forcemerge
			
			文档冲突：
				悲观并发控制
					_version版本控制 （老版本才有，新版本if_seq_no和if_primary_term）
				乐观并发控制


七、Redis
	1、数据类型
		String ：简单动态字符串
		List ：双向链表，压缩列表
		Hash(Map)：压缩列表、哈希表
		Set ：整数数组、哈希表
		ZSet（Sorted）：压缩列表、跳表 
		Bitmap
	2、 数据结构
		键和值用什么结构组织：
			全局哈希表：保存了所有的键值对（entry）
			全局希表哈希冲突: 链式哈希表，用指针连接
			refresh：增加现有的哈希桶数量，让逐渐增多的entry元素能够在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。
				先分配翻倍空间哈希表，拷贝复制原来的哈希表到新的哈希表；释放原来的哈希表；
				渐进式refresh：
					拷贝数据Redis还可以正常处理客户端请求，没处理一个请求，从哈希表1的索引第一个索引位置开始，顺带着将这个索引位置上的所有
					entries拷贝到哈希表2中；等处理下一个请求时，顺带拷贝哈希表1中的下一个索引位置的entries。

		集合数据操作效率：
			整数数组、链表、压缩列表：O(n)
			哈希表查找复杂度：O(1)
			跳表：O(logN)
	
	3、高性能IO模型
		Redis单线程，Redis的网络IO和键值对读写是由一个线程来完成的。
			Redis大部分操作都是在内存上完成的，采用了高效的数据结构，如哈希表，跳表等。
			Redis采用了多路复用机制，使其在网络IO操作中能并发处理大量的客户端请求，实现高吞吐率。处理多个IO流
		Redis单线程瓶颈：
			操作bigkey，写入或删除都需要耗费大量的IO。
			使用复杂度过高的命令。
			大量的key集中过期：Redis的过期机制是在主线程中执行，大量key集中过期会导致处理一个请求时耗时都在删除过期key，耗时变长。
			淘汰策略：淘汰策略也是在主线程中执行的，当内存超过Redis的内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长。
			AOF刷盘开启Always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能。
			主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这个瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久。
			并发量非常大：单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。 
	
	4、AOF日志：宕机了Redis如何避免数据丢失
		AOF： 先写内存再写磁盘，避免额外的检查开销，不会阻塞当前写操作的执行。（aof_buffer）

		aof同步策略：写入系统内核缓冲区时机（操作系统缓存）和刷写到磁盘的时机
			always：总是，每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；
			everysec：每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；
			no：不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。

		aof：重写策略，aof记录的是日志，故障恢复时需要执行一遍日志。
			写时复制策略：主进程发生修改时才复制，命令写到aof重写缓冲区、和aof缓冲区

			fork一个Begrewwriteof线程来执行重写操作，当主进程发生写操作时，进行写时复制，重写完后与重写缓冲区合并。
		
		rdb：快照 
		 	bgsave：每隔一段时间执行一次
				save 900 1
				save 300 10
				save 60 10000
			
			rdb执行时会阻塞线程？
				save在主进程中进行快照，会进行阻塞。
				bgsave创建一个子线程进行快照。
			
			rdb进行快照时数据能够被修改？
				bgsave可以，fork一个子线程，如果主进程没有修改都是读对子线程没有影响，若发生了写操作则会进行写时复制。
				不会阻塞主进程，主进程的修改只能等下一次bgsave才可以生成rdb。
			
			为什么会有混合持久化？
				极端情况下会丢失进行rdb快照时的写操作数据，所以混合使用可保证数据不丢失。
			
			如何实现混合持久化？
				aof-use-rdb-preamble yes
				aof重写时，fork出来的子线程会先将rdb文件复制到aof文件，主线程新的写操作会写到重写缓冲区，待到rdb复制完成再将重写缓冲区的数据写到aof文件中。
			
		主从复制
			全量复制：
				主写从读 ：replicaof ip （主服务器）
				第一次同步：
					从服务器进行复制，主线程进行写操作，如何保持数据一致性。
						将写操作写入到Replication Buffer 

			从服务器（代理复制分摊主服务器同步压力）
				replicaof 代理从服务器 6379
			
			增量复制：
				repl_backlog_buffer(环形缓冲区)
					主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写到环形缓冲区，缓存中会保存最近传播的写命令。
					若是从服务器读的数据还在缓冲区，则增量复制，否则全量复制。
				
				replication_offset: master_repl_offset 写的位置 slave_repl_offset 读的位置
		
		哨兵机制：为了主从节点故障转移
			哨兵机制如何工作？
				监控
					哨兵每一秒都会给所有的主从节点发送一个ping命令，如果没有再规定时间响应就会判断为主观下线。
					客观下线：为了避免主节点误判 quorum：哨兵个数的1/2 + 1
				选主(由leader节点进行故障转移)
					候选者：判断主节点客观下线的节点，通过其他节点投票数达到quorum。
					候选者选举为leader：
						候选者发送命令进行选举
						哨兵进行投票，每个哨兵只能投一票。
							拿到半数以上的票数。
							拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。
				通知
			
			主从故障转移
				选举出主节点
					从已下线的主节点的从节点中选举，先把网络不好的给过滤掉。
						down-after-milliseconds * 10 配置项，其down-after-milliseconds 是主从节点断连的最大连接超时时间。
					优先级：优先级越小排名越靠前。
					复制进度：优先级相同，查看复制的下标，选择复制数据多的。
					节点ID：选择节点ID较小的。
				将从节点指向新主节点
					让属下从节点指向新主节点，salveof命令。
				通知客户的主节点已更新
					通过 Redis 的发布者/订阅者机制来实现。
						命令： 
							+sdown 进入主观下线、-sdown 退出主观下线
							+odown 进入客观下线、-odown 退出客观下线
							+slave-reconf-sent 哨兵发送SLAVEOF命令重新配置从库
							+slave-reconf-inprog 从库配置了新主库，但尚未进行同步
							+slave-reconf-done 从库配置了新主库，且和新主库完成同步
							+switch-master 主库地址发生变化
				将旧主节点变为从节点


				哨兵集群如何组成？
					通过通知订阅的方式，主节点上有一个名为_sentinel_:hell0的频道
				
				哨兵如何获取从节点信息？
					主节点知道所有从节点的信息，所以哨兵会每10秒一次的频率向主节点发送info命令来获取所有从节点信息。 
			
			切片集群模式（一个切片集群由16384个哈希槽）
				手动分配
				平均分配
			
			集群脑裂
				什么是脑裂：出现两个主节点
				脑裂丢数据：旧主节点会清空之前的数据去同步新的主节点的数据
				解决：
					min-slaves-to-write X:主节点必须要有至少X个节点连接，如果小于这个数，主节点会禁止写数据。
					min-slaves-to-lag X: 主从数据复制和同步的延迟不能超过X秒，如果超过，主节点会禁止写数据。
			
			Redis过期删除和内存淘汰
				惰性删除：不制动删除过期键，每次从数据库访问Key时，都检测Key是否过期，如果过期则删除该Key。
				定期删除：定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。

				持久化对过期键删除
					aof 
						写入阶段：过期键被删除后，Redis会向aof文件中，Redis 会向AOF文件追加一条 DEL 命令来显式地删除该键值。
						重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。
					rdb 
						写入阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。
						载入阶段：
							主服务器：内存中过期的键不会写入到rdb。
							从服务器：会同步写到rdb。

			内存淘汰策略
				LRU 
				LFU ：记录使用的频次，最少使用次数。						

八、MySQL
	1、基础	
		一条SQL是如何执行的？
			客户端 Client  
			服务端	Server 
				缓存、解析器、预处理器、优化器、执行器
		一行数据是如何存储的？
			文件：opt、frm、ibd
			表空间文件结构
				Segment 
				Extent ：1MB
				Page 16K 
				Row 
			存储格式：
				Compact 
					变长字段列表
					NULL值列表
					记录头信息
						next_record:指向下一条记录的头信息和真实数据之间的位置。
					记录真实数据：
						row_id:没有指定主键或者唯一键自动生成
						trx_id:事务id
						roll_pointer:回滚指针
				varchar(n) ：n的最大取值
					MySQL规定除了TEXT、BLOBs这种大对象类型外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节
					n代表最多存储的字符数量，并不是字节大小；ascii中，1字符占用1字节；UTF-8下1个字符需要3字节。
				
				varchar(n)存储： 
					真实数据
					真实数据占用的字节数
						变长字段：小于等于255，1字节；大于255，2字节。
						定长字段：
					NULL标识：如果不允许为NULL，这部分不需要
				
			行溢出（指的是页保存不了一行数据）
				一个页大小16KB，varchar(n)最多可以存储65535字节。
				发生行溢出，多的数据存到另外的溢出页中。
	
	2、索引
		索引分类
			数据结构：B+tree索引、Hash索引、Full-text索引
			物理存储: 聚簇索引（主键索引）、二级索引（辅助索引）
			字段特性：主键索引、唯一索引、普通索引、前缀索引
			字段个数：单列索引、联合索引
		
		数据结构分类的索引：
			B+tree索引 
				聚簇索引：存储数据的索引
				覆盖索引：通过二级索引就可以找到查找的值，select id from table_name where name='wither'
				回表：需要查两次索引，一次为二级索引B+tree，一次为主键索引

			为什么使用B+tree作为底层索引数据结构？
				B+tree vs B Tree 
				B+tree VS 二叉树
				B+tree VS Hash 

		物理存储分类
			聚簇索引：存储数据 
			二级索引：存储主键
		
		字段特性分类：
			唯一索引：一张表可以有多个，索引列的值必须唯一，但是允许有空值
			普通索引：index(field)
			前缀索引：index(filed(length))
		
		字段个数：
			单列索引
			联合索引
		

		什么时候需要索引、什么时候不需要？
			需要索引：
				字段有唯一性限制，比如商品编码
				经常用于where查询条件的字段，如果查询字段不是一个，可以建立联合索引
				经常用于order by 和 group by的字段，这样排序查询时就不需要再去做一次排序，因为建立索引时已经是排好序的。
			
			不需要建立索引：
				字段值大量重复数据
				表数据太少
				经常更新的字段
			
		优化索引：
			前缀索引优化
				为了减小索引字段的大小
				限制：
					order by 无法使用前缀索引
					无法把前缀索引用作覆盖索引
			
			覆盖索引优化
				联合索引可以避免回表
			
			主键索引最好是自增
				数据的存放是按照数据的主键顺序存放的，数据插入时，数据库会根据主键插入到指定的叶子节点中。
				自增的主键每次插入一条新记录，都是追加操作，不需要重新移动数据。
				非自增主键插入可能是在数据页的中间，这就可能要移动其他数据来满足数据的插入，甚至需要复制数据到另外一个页。
			
			索引最好设置为NOT NULL 
				count 会忽略NULL值
				至少需要1字节空间存储NULL值列表

			防止索引失效
		
		InnoDB是如何存储数据
			文件的存储是按照数据页进行存储的
				文件头（38字节） File Header ：页的信息
				页头（56字节） Page Header 
				最大最小记录（26字节）
				用户记录
				空闲空间（Free Space）
				页目录 （Page Directory）：存储用户记录的相对位置，索引作用
				文件尾 （File Tailer 8字节）：检验页是否完整
			页目录：
				数据页中主键按照主键id形成链表
				数据页中有页目录，形成槽，进行索引。可以使用二分查找快速定位
		
		InnoDB为什么使用B+树？
			单点查询
			插入和删除效率
			范围查询

		索引失效
			对索引使用了左或左右模糊查询
			对索引使用了函数
			对索引使用了表达式计算
			对索引隐式类型转换
			联合索引非最左匹配
			where子句中的or

		count() 和 count(*)
			count(1)或者count(fieldname)不统计null值
	
	事务篇
		事务的隔离级别是如何实现的
			事务特性
				原子性
				一致性
				隔离性
				持久性
			并发事务引发的问题
				脏读
				幻读
				不可重复读
			事务的隔离级别
				读未提交
				读提交 
				可重复读
					InnoDB支持
						快照读（普通select）
							通过MVCC方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，
							即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
						当前读（select ... for update）
							通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，
							如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
				串行化
		
		开启事务：
			begin/start traction 
			start transaction with consistent snapshot
		
		Read view数据快照（使用）
						读提交：每个语句执行前都会重新生成一个Read View
						可重复读：启动事务时，生成一个Read View，然后整个事务期间都在使用这个Read View。
		
		快照在MVCC中工作
			trx_id:
				creator_trx_id：创建Read View时当前数据库中应该给下一个事务的id值。
				m_ids：创建Read View时，当前数据库中活跃事务的事务id列表，活跃事务：启动了但还没提交的事务。
				min_trx_id：活跃事务中的事务id最小值
				max_trx_id: 创建Read View时当前数据库中应该给下一个事务的id值。
			roll_pointer:
				每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到undo日志中，然后这个隐藏列是个指针，指向每个旧版本记录，可以通过此找到修改前的记录。
		
		可重复读如何工作
			启动事务时，创建一个Read View 
			判断trx_id是否在min_trx_id 和 max_trx_id之间不
		读提交如何工作
			每次读取时生成一个新的Read View 
		
		快照读如何避免幻读
			在执行第一个查询语句后，就会创建一个Read View，后续的查询语句利用这个Read View，
			通过这个Read View就可以在undo log版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的。
		
		当前读如何避免幻读
			间隙锁，对表的记录加上next-key lock

锁 
	锁类型
		全局锁
			如何避免备份全局锁影响业务？
				数据库引擎支持的事务是可重复读隔离级别的，在备份数据库前先开启事务，会先创建Read View，然后整个事务期间都在使用这个Read
				View，而且由于MVCC支持，备份期间依然可以对数据进行更新。
			mysqldump备份数据库工具

		表级锁
			表锁 ：共享锁、独占锁
			元数据锁（MDL）：事务提交才会关闭
			意向锁：
				为了快速判断表里是否有记录被加锁。如果没有意向锁，那么加独占表锁时就需要遍历表里的所有记录，查看是否存在独占锁，效率会很慢。
				select ... lock in share mode; select ... for update;
			AUTO-INC锁：自增锁
				
		行级锁
			Record Lock：记录锁
			Gap Lock：间隙锁（存在于可重复读级别）
				两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系。
			Next-Key Lock：临键锁
				如果一个事务获取了X型的next-key Lock，另外一个事务在获取相同的范围的X型锁时会被阻塞
			插入意向锁：（判断插入位置是否被其他事务加了间隙锁（next-key也包含了间隙锁））
	
	MySQL怎么加锁？
		加行级锁
			读操作：
				普通的select语句是不会对记录加锁，属于快照读，通过MVCC实现。
				锁定读会加锁：
					select ... lock in share mode;
					select ... for update;
			独占锁（update、delete操作）
		
		怎么加行级锁？
			加锁的对象是索引，加锁的基本单位是next-key lock，它是由记录锁和间隙锁组成的，
			next-key是前开后闭区间，而间隙锁是前开后开区间。
		
			唯一索引等值查询
				记录存在 -- 该记录的索引中next-key lock会退化成记录锁。
				记录不存在 -- 在索引树中找到第一条大于该查询记录的记录后，将该记录的索引中next-key lock退化成间隙锁。
			
			唯一索引范围查询
				大于等于范围查询
					next-key lock 退化成记录锁
				小于或者小于等于查询
					条件值不存在：next-key 退化成间隙锁
					条件值存在：	
	
	update没加索引会锁全表？
		关键是这条语句执行过程中，优化器最终选择的是索引扫描，还是全表扫描，如果是全表扫描就会对全表的记录加锁。
	







Java
	抽象类
	继承
	接口
	多态
	final关键字
	static关键字
	内部类
		成员内部类
		局部内部类
	匿名对象
	代码块
	包

	Java网络编程
		IO分类
			BIO、NIO、AIO
		IO工作原理
			用户态
			内核态提供函数：read()、write()
			流程：
				首先在网络的网卡上或本地存储设备中准备数据，然后调用read()函数。
				调用read()函数后，将数据读入到内核缓冲区中。
				读取完后向CPU发起一个中断信号，通知CPU对数据进行后续的处理。
				CPU将内核中的数据写入对应的程序缓冲区或网格Socket接收缓冲区中。
				数据全部写到缓冲区后，应用程序开始对数据进行实际的处理。
			CPU中断信号方式：（告知CPU读取数据已经完成，异步通信？）
				忙等待方式
				中断驱动方式
				DMA直接存储器方式
			内核态和用户态
				将一些危险操作给屏蔽，不让用户去操作。
			同步和异步
			阻塞与非阻塞
		
		IO分类 
			同步阻塞式IO BIO ：当系统调用read()时，该线程会阻塞。
				多线程处理并发，或者使用线程池。（Tomcat）
			
			同步非阻塞IO NIO 
				系统调用read()时，数据为未就绪状态，用户程序轮询内核，询问是否数据已准备好。
				问题：
					频繁去询问数据是否已准备好，CPU开销比较大
			
			多路复用
				内核准备好，再去询问内核数据是否准备好。
				selector选择器
				连接通过channel注册到选择器上
			
			信号驱动模型
				数据准备完成返回一个signo信号
			异步非阻塞IO-AIO 
				借助信号驱动模型
	
	JUC编程
		一、集合不安全
			List、Set不安全
			Vector、synchronizedList、CopyOnWriteArrayList安全
				Vector、synchronizedList使用synchronized锁，CopyOnWriteArrayList使用ReentrantLock锁
			HashSet底层其实就是HashMap，value作为key，key=PRESENT。
				map.put(e, PRESENT)
			HashMap 
				安全map
					Collections.synchronizedMap(new HashMap(...)); 
					Map<String, String> map = new ConcurrentHashMap<>();
		
		二、Callable
			1、特点
				可以有返回值
				可以抛出异常
				call()方法实现
				get()方法阻塞主线程返回消息
			2、val future = CompletableFuture.supplyAsync()
			   future.whenComplete()
		
		三、辅助类
			1、CountDownLatch
				计数器归0，countDownLatch.await() 就会被唤醒，继续执行
			2、CyclicBarrier
			3、Semaphore
				信号量 
		
		四、读写锁
			lock = ReadReetrnLock()
			lock.writeLock().lock()
			lock.readLock().lock()
		
		五、函数接口
			四大接口
				lambda表达式
				链式编程
				函数式接口
				Stream流式计算
		
		六、ForkJoin （继承ForkJoinTask）
			>什么是ForkJoin？ ForkJoin在JDK1.7中，并行执行任务！提高效率，大数据量！！！
			> ForkJoin的特点：工作窃取，里面维护的是一个双端队列

		七、异步调用：CompletabeFuture
			异步执行
			成功回调
			失败回调
			CompletabeFuture.runAsync() 没有返回值
			CompletabeFuture.supplyAsync() 有返回值
		
		八、JMM

		九、Volatile
			保证可见性
			不保证原子性
			禁止指令重排

十、SQL
	1、窗口函数
		1.row_number()、rank()、dense_rank()
	  	row_number():序号是连续的，不会重复
  	  	rank():排序相同时会重复，序号会断
	  	dense_rank():序号连续，会重复
		2.cume_dist(): 小于等于当前rank/总行数
	  	percent_rank():当前rank() / rows - 1
		3.lag()、lead()前后函数
		4.头尾函数
			first_val()、last_val()

十一、算法
	1、排序算法
		1.冒泡排序
		2.快速排序
		3.归并排序
		4.二叉树
			前序、中序、后序
			广度遍历、深度遍历
		
	2、设计模式
	 




				








Kafka消费者消费偏移量自动按照什么机制来提交消费位移的？
	auto.offset.commit
		按照时间间隔提交 5ms 
		enable.auto.commit :是否自动提交offset功能，默认true。
		auto.commit.interval.ms:自动提交offset得时间间隔，默认5s。
	
	提交到哪？
		手动提交：异步提交 consumer.commitAsync() 同步提交 consumer.commit()
		topic: __consumer_offset 

Kafka的消费者是否支持事务？
	不存在，只有Producer支持

			


		  
	





		






				

							







		



		




			



						


					



					
				











			
						
						
						
				
			






















